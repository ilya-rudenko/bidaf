{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 1246668,
     "sourceType": "datasetVersion",
     "datasetId": 715814
    },
    {
     "sourceId": 7195830,
     "sourceType": "datasetVersion",
     "datasetId": 4160853
    },
    {
     "sourceId": 7194099,
     "sourceType": "datasetVersion",
     "datasetId": 4160358
    }
   ],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import re, os, string, typing, gc, json\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:44.045935Z",
     "iopub.execute_input": "2023-12-14T00:02:44.046318Z",
     "iopub.status.idle": "2023-12-14T00:02:52.727444Z",
     "shell.execute_reply.started": "2023-12-14T00:02:44.046286Z",
     "shell.execute_reply": "2023-12-14T00:02:52.726610Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_pickle('/kaggle/input/bidaf-preprocessed-dataset/bidaftrain.pkl')\n",
    "valid_df = pd.read_pickle('/kaggle/input/bidaf-preprocessed-dataset/bidafvalid.pkl')\n",
    "\n",
    "with open('/kaggle/input/bidaf-preprocessed-dataset/bidafw2id.pickle','rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    "with open('/kaggle/input/bidaf-preprocessed-dataset/bidafc2id.pickle','rb') as handle:\n",
    "    char2idx = pickle.load(handle)\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:52.729041Z",
     "iopub.execute_input": "2023-12-14T00:02:52.729563Z",
     "iopub.status.idle": "2023-12-14T00:02:56.641177Z",
     "shell.execute_reply.started": "2023-12-14T00:02:52.729533Z",
     "shell.execute_reply": "2023-12-14T00:02:56.640157Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SquadDataset:\n",
    "    def __init__(self, data, batch_len):\n",
    "        \n",
    "        self.batch_len = batch_len\n",
    "        \n",
    "        data = [data[i:i+self.batch_len] for i in range(0, len(data), batch_len)]\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
    "        \n",
    "        char_vec = torch.ones(max_sent_len, max_word_len) \n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)\n",
    "        \n",
    "        return char_vec    \n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for batch in self.data:\n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "                \n",
    "            max_word_ctx = 0\n",
    "            for context in batch.context:\n",
    "                for word in nlp(context, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ctx:\n",
    "                        max_word_ctx = len(word.text)\n",
    "            \n",
    "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 0\n",
    "            for question in batch.question:\n",
    "                for word in nlp(question, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ques:\n",
    "                        max_word_ques = len(word.text)\n",
    "            \n",
    "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            \n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:56.642629Z",
     "iopub.execute_input": "2023-12-14T00:02:56.642933Z",
     "iopub.status.idle": "2023-12-14T00:02:56.660958Z",
     "shell.execute_reply.started": "2023-12-14T00:02:56.642906Z",
     "shell.execute_reply": "2023-12-14T00:02:56.660057Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = SquadDataset(train_df, 8)\n",
    "valid_dataset = SquadDataset(valid_df, 8)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:56.663950Z",
     "iopub.execute_input": "2023-12-14T00:02:56.664651Z",
     "iopub.status.idle": "2023-12-14T00:02:56.754120Z",
     "shell.execute_reply.started": "2023-12-14T00:02:56.664615Z",
     "shell.execute_reply": "2023-12-14T00:02:56.753321Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test = next(iter(train_dataset))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:56.755325Z",
     "iopub.execute_input": "2023-12-14T00:02:56.755677Z",
     "iopub.status.idle": "2023-12-14T00:02:59.789202Z",
     "shell.execute_reply.started": "2023-12-14T00:02:56.755645Z",
     "shell.execute_reply": "2023-12-14T00:02:59.788153Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Character Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CharacterEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
    "        \n",
    "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.dropout(self.char_embedding(x))\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.relu(self.char_convolution(x))\n",
    "        x = x.squeeze() \n",
    "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
    "        x = x.view(batch_size, -1, x.shape[-1])\n",
    "\n",
    "        return x        "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:59.790410Z",
     "iopub.execute_input": "2023-12-14T00:02:59.790675Z",
     "iopub.status.idle": "2023-12-14T00:02:59.799337Z",
     "shell.execute_reply.started": "2023-12-14T00:02:59.790652Z",
     "shell.execute_reply": "2023-12-14T00:02:59.798232Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Highway"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HighwayNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow_value = F.relu(self.flow_layer[i](x))\n",
    "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
    "            \n",
    "            x = gate_value * flow_value + (1 - gate_value) * x\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:59.800603Z",
     "iopub.execute_input": "2023-12-14T00:02:59.800953Z",
     "iopub.status.idle": "2023-12-14T00:02:59.812879Z",
     "shell.execute_reply.started": "2023-12-14T00:02:59.800922Z",
     "shell.execute_reply": "2023-12-14T00:02:59.812113Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Contextual Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ContextualEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.highway_net = HighwayNetwork(input_dim)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        highway_out = self.highway_net(x)\n",
    "        \n",
    "        outputs, _ = self.lstm(highway_out)\n",
    "        \n",
    "        return outputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:59.813988Z",
     "iopub.execute_input": "2023-12-14T00:02:59.814330Z",
     "iopub.status.idle": "2023-12-14T00:02:59.824773Z",
     "shell.execute_reply.started": "2023-12-14T00:02:59.814307Z",
     "shell.execute_reply": "2023-12-14T00:02:59.823873Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "prep_weights_matrix = np.load(\"/kaggle/input/bidaf-preprocessed-dataset/bidafglove_tv.npy\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:02:59.825781Z",
     "iopub.execute_input": "2023-12-14T00:02:59.826045Z",
     "iopub.status.idle": "2023-12-14T00:03:00.436965Z",
     "shell.execute_reply.started": "2023-12-14T00:02:59.826021Z",
     "shell.execute_reply": "2023-12-14T00:03:00.436165Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BiDAF(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
    "                 kernel_size, ctx_hidden_dim, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
    "                                                      num_output_channels, kernel_size)\n",
    "        \n",
    "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
    "        \n",
    "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = prep_weights_matrix\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "        \n",
    "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
    "        ctx_len = ctx.shape[1]\n",
    "        \n",
    "        ques_len = ques.shape[1]\n",
    "        \n",
    "        ctx_word_embed = self.word_embedding(ctx)\n",
    "        \n",
    "        ques_word_embed = self.word_embedding(ques)\n",
    "        \n",
    "        ctx_char_embed = self.character_embedding(char_ctx)\n",
    "        \n",
    "        ques_char_embed = self.character_embedding(char_ques)\n",
    "        \n",
    "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
    "        \n",
    "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
    "        \n",
    "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
    "        \n",
    "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
    "        \n",
    "        ## CREATE SIMILARITY MATRIX\n",
    "        \n",
    "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
    "        \n",
    "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        \n",
    "        elementwise_prod = torch.mul(ctx_, ques_)\n",
    "        \n",
    "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
    "        \n",
    "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
    "        \n",
    "        ## CONTEXT2QUERY\n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=-1)\n",
    "        \n",
    "        c2q = torch.bmm(a, ques_contextual_emb)\n",
    "        \n",
    "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
    "        \n",
    "        b = b.unsqueeze(1)\n",
    "        \n",
    "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
    "        \n",
    "        q2c = q2c.repeat(1, ctx_len, 1)\n",
    "        \n",
    "        G = torch.cat([ctx_contextual_emb, c2q, \n",
    "                       torch.mul(ctx_contextual_emb,c2q), \n",
    "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n",
    "        \n",
    "        M, _ = self.modeling_lstm(G)\n",
    "        \n",
    "        ## OUTPUT LAYER\n",
    "        \n",
    "        M2, _ = self.end_lstm(M)\n",
    "        \n",
    "        # START PREDICTION\n",
    "        \n",
    "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
    "        \n",
    "        p1 = p1.squeeze()\n",
    "            \n",
    "        # END PREDICTION\n",
    "        \n",
    "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
    "          \n",
    "        return p1, p2\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:03:00.440031Z",
     "iopub.execute_input": "2023-12-14T00:03:00.440352Z",
     "iopub.status.idle": "2023-12-14T00:03:00.457961Z",
     "shell.execute_reply.started": "2023-12-14T00:03:00.440325Z",
     "shell.execute_reply": "2023-12-14T00:03:00.457051Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "char_vocab_dim = len(char2idx)\n",
    "emb_dim = 100\n",
    "char_emb_dim = 8\n",
    "ouput_channels = 100\n",
    "kernel_size = (8,5)\n",
    "hidden_dim = 100\n",
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.002465Z",
     "iopub.execute_input": "2023-12-14T00:08:17.003354Z",
     "iopub.status.idle": "2023-12-14T00:08:17.008246Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.003318Z",
     "shell.execute_reply": "2023-12-14T00:08:17.007250Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BiDAF(char_vocab_dim, \n",
    "              emb_dim, \n",
    "              char_emb_dim, \n",
    "              ouput_channels, \n",
    "              kernel_size, \n",
    "              hidden_dim, \n",
    "              device).to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.236672Z",
     "iopub.execute_input": "2023-12-14T00:08:17.237047Z",
     "iopub.status.idle": "2023-12-14T00:08:17.289007Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.237016Z",
     "shell.execute_reply": "2023-12-14T00:08:17.288176Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adadelta(model.parameters())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.420003Z",
     "iopub.execute_input": "2023-12-14T00:08:17.420704Z",
     "iopub.status.idle": "2023-12-14T00:08:17.425041Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.420668Z",
     "shell.execute_reply": "2023-12-14T00:08:17.424150Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_dataset):\n",
    "\n",
    "    train_loss = 0.\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device), char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "\n",
    "        s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.581162Z",
     "iopub.execute_input": "2023-12-14T00:08:17.581545Z",
     "iopub.status.idle": "2023-12-14T00:08:17.590385Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.581514Z",
     "shell.execute_reply": "2023-12-14T00:08:17.589267Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def valid(model, valid_dataset):\n",
    "   \n",
    "    valid_loss = 0.\n",
    "    \n",
    "    f1 = 0.\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in tqdm(valid_dataset):\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device), char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            \n",
    "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "\n",
    "    \n",
    "    f1 = evaluate(predictions)\n",
    "    return valid_loss/len(valid_dataset), f1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.706014Z",
     "iopub.execute_input": "2023-12-14T00:08:17.706684Z",
     "iopub.status.idle": "2023-12-14T00:08:17.718349Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.706651Z",
     "shell.execute_reply": "2023-12-14T00:08:17.717425Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(predictions):\n",
    "    with open('/kaggle/input/squad-dataset/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return f1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.866189Z",
     "iopub.execute_input": "2023-12-14T00:08:17.867020Z",
     "iopub.status.idle": "2023-12-14T00:08:17.874362Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.866984Z",
     "shell.execute_reply": "2023-12-14T00:08:17.873446Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:17.997040Z",
     "iopub.execute_input": "2023-12-14T00:08:17.997440Z",
     "iopub.status.idle": "2023-12-14T00:08:18.008348Z",
     "shell.execute_reply.started": "2023-12-14T00:08:17.997408Z",
     "shell.execute_reply": "2023-12-14T00:08:18.007351Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    valid_loss, f1 = valid(model, valid_dataset)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "            'f1':f1,\n",
    "            }, 'bidaf_run_{}.pth'.format(epoch))\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    f1s.append(f1)\n",
    "\n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:08:18.161414Z",
     "iopub.execute_input": "2023-12-14T00:08:18.162127Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T00:07:21.031668Z",
     "iopub.status.idle": "2023-12-14T00:07:21.032176Z",
     "shell.execute_reply.started": "2023-12-14T00:07:21.031909Z",
     "shell.execute_reply": "2023-12-14T00:07:21.031934Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
